---
title: "Métodos de Gran Escala. Proyecto 2"
author: "Felipe Gerard"
date: "16 de mayo de 2015"
output: html_document
---

UFO
----------------------------------------------------------------------------


## 1. Obtención de los datos

La fuente de los datos de avistamientos está [aquí](FALTA!). En un proyecto anterior habíamos utilizado principalmente _R_ para obtener la información de los avistamientos. Sin embargo, en esta estrategia no es muy efectiva porque no podemos tener más de 5 ó 10 instancias pidiendo información. En esta versión optamos por usar `curl` y `parallel` para bajar los HTMLs tanto de las tablas como de las descripciones largas.

Ya con la información abajo, utilizamos el comando `pup` para ayudarnos a procesar el HTML y finalmente el paquete `rvest` de _R_ para pasarlo todo a formato tabla. Parte de este proceso fue el pegado de las descripciones largas a las tablas como una columna más, para lo que requerimos limpiar los textos de saltos de línea, etc.


## 2. Limpieza

Para la limpieza optamos por utilizar _PostgreSQL_. Si bien la base de UFO es pequeña y puede ser manejada en _R_ sin ningún problema, optamos por pasarla por todo el proceso como práctica para la base de GDELT. Optamos por utilizar un proceso ELT, del cuál la etapa anterior corresponde a la extracción. Después de ese proceso ya teníamos la información en valores separados por pipes. Sin embargo, la base seguía estando sumanente sucia. Entonces seguía la etapa de carga a la base de datos.

Para esta parte simplemente subimos la información a una tabla de una sola columna de formato `varchar`. El objetivo era procesarla y dejarla lista para el análisis arriba en el PostgreSQL. Optamos por particionar la tabla anualmente. Para que pudiéramos utilizar la tabla como si fuera una sola, añadimos _triggers_ para controlar que las inserciones en la tabla maestra se hicieran en realidad en la del año correspondiente (o en la tabla _overflow_ en caso de no haber una tabla apropiada).

Para la limpieza lo que hicimos fue _castear_ las columnas de manera segura a los tipos apropiados, por un lado, y por otro extraer la información relevante y transformarla a un formato útil. Por ejemplo, la duración no venía con un formato específico, así que tuvimos que extraer la cantidad y las unidades para transformarlo a segundos. Ya con la información lista, dado que como mencionábamos no son tantos datos, la pasamos a _R_ para el análisis. A la hora de la lectura hubo que hacer un poco más de limpieza porque había unos pocos renglones que habían llegado mal. Y entonces podemos pasar a la etapa del análisis descriptivo.


## 3. Análisis descriptivo

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(ggmap)
library(RColorBrewer)
setwd("/Users/Felipe/big-data/alumnos/felipegerard/proyecto2")
# Leemos la base que limpiamos en postgres
ufo <- read.table('output/ufo_usa.psv', header = T, sep = '|', quote = "", stringsAsFactors = F, fill = T,
                  colClasses = rep('character',18))

long_vars <- c(15,17,18) # Columnas de texto largas
################################################################
# Limpieza aparte de lo de postgres
################################################################

#data.frame(sapply(ufo,class))
for(x in c('year','month','day','weekday','number','seconds')){
  ufo[[x]] <- as.numeric(ufo[[x]])
}
ufo <- filter(ufo, nchar(origin) < 20)  %>% # Algunos renglones venían mal
  mutate(id = row_number())
dim(ufo)
```

```{r, message=FALSE, warning=FALSE}
### Para lo espacial
state_abbr <-
  matrix(c("alabama", "AL", "alaska", "AK", "arizona", "AZ", "arkansas", "AR", "california", "CA",
           "colorado", "CO", "connecticut", "CT", "district of columbia", "DC", "delaware", "DE", "florida", "FL", "georgia", "GA",
           "hawaii", "HI", "idaho", "ID", "illinois", "IL", "indiana", "IN", "iowa", "IA", "kansas", "KS",
           "kentucky", "KY", "louisiana", "LA", "maine", "ME", "maryland", "MD", "massachusetts", "MA",
           "michigan", "MI", "minnesota", "MN", "mississippi", "MS", "missouri", "MO", "montana", "MT",
           "nebraska", "NE", "nevada", "NV", "new hampshire", "NH", "new jersey", "NJ", "new mexico", "NM",
           "new york", "NY", "north carolina", "NC", "north dakota", "ND", "ohio", "OH", "oklahoma", "OK",
           "oregon", "OR", "pennsylvania", "PA", "rhode island", "RI", "south carolina", "SC",
           "south dakota", "SD", "tennessee", "TN", "texas", "TX", "utah", "UT", "vermont", "VT",
           "virginia", "VA", "washington", "WA", "west virginia", "WV", "wisconsin", "WI", "wyoming", "WY"),
         ncol=2, byrow=T) %>%
  data.frame(stringsAsFactors = F)
names(state_abbr) <- c('region', 'state')

```

Como queríamos sacar conclusiones sobre los avistamientos en EUA, entonces filtramos la base completa únicamente tomando los registros que tuvieran un valor válido según [esta lista](http://www.50states.com/abbreviations.htm#.VVeoSs4_Hkh). En adelante nos referiremos a esta base filtrada con el nombre UFO, y sobre ella haremos todos los análisis.

Dimensiones de la base:
```{r}
dim(ufo)
```

Columnas y sus tipos:
```{r}
cols <- data.frame(columna=names(ufo), tipo=sapply(ufo, class), row.names = NULL)
cols$descripcion <- c('Archivo de origen del registro',
                      'ID único del registro',
                      'Fecha y hora del avistamiento',
                      'Año','Mes','Día','Día de la semana (0=domingo, 6=sábado)',
                      'Ciudad en la que fue avistado el UFO',
                      'Estado en el que fue avistado el UFO',
                      'Forma del objeto',
                      'Duración del avistamiento',
                      'Número leído del campo de duración',
                      'Unidades leídas del campo de duración',
                      'Segundos que duró el avistamiento (número*segundos/unidades)',
                      'Resumen de la descripción',
                      'Fecha y hora de subida a la base',
                      'URL de la descripción larga',
                      'Descripción larga')
cols
```

Primeras y últimas observaciones (sin las descripciones ni la URL). Tal parece que las observaciones modernas tienen un mejor formato de duración:
```{r}
head(ufo %>% select(-summary, -long_description, -description_url))
tail(ufo %>% select(-summary, -long_description, -description_url))
```

Ejemplos de descripción corta (`summary`) y descripción larga (`long_description`). Los `<n>` son saltos de línea.
```{r}
ufo$summary[1]
ufo$long_description[1]
```

Resumen estadístico (primero variables numéricas y luego variables de texto):
```{r}
col_classes <- sapply(ufo, class) %>% as.character
cols_char <- which(col_classes == 'character')
cols_num <- which(col_classes != 'character')
summary(ufo[cols_num])
apply(ufo[cols_char], 2, function(x) c('length'=length(x),
                                       '# empty'=sum(x == ''),
                                       '% empty'=mean(x == ''))) %>%
  round(2) %>%
  format(scientific = F)
```


## 4. Análisis

En esta sección responderemos varias preguntas simples y haremos análisis estadísticos de la base.

### 4.1 Preguntas cerradas

**Primer avistamiento por estado**
```{r}
ufo %>%
  filter(!is.na(year)) %>%
  group_by(state) %>%
  arrange(state, year) %>%
  filter(row_number() == 1) %>%
  dplyr::select(state, date_time, city, shape, duration, seconds, posted)
```


**Primer avistamiento por forma**
```{r}
ufo %>%
  filter(shape != '') %>%
  group_by(shape) %>%
  arrange(shape, year) %>%
  filter(row_number() == 1) %>%
  dplyr::select(state, date_time, city, shape, duration, seconds, posted)
```

**Número de avistamientos por año**
```{r}
yearly_sightings <- ufo %>%
  filter(date_time != '', year < 2015) %>% # Este año no está completo
  group_by(year) %>%
  summarise(count = n()) %>%
  arrange(year)
ggplot(yearly_sightings, aes(year,count)) +
  geom_line() +
  geom_point()
```

**Promedio mensual de avistamientos**
```{r}
ufo %>%
  filter(date_time != '') %>%
  group_by(year, month) %>%
  summarise(count = n()) %>%
  ungroup %>%
  summarise(monthly_mean = mean(count))
```

**Promedio anual de avistamientos**
```{r}
ufo %>%
  filter(date_time != '') %>%
  group_by(year) %>%
  summarise(count = n()) %>%
  ungroup %>%
  summarise(yearly_mean = mean(count))
```

**Promedio de avistamientos por mes**
```{r}
monthly_sightings <- ufo %>%
  filter(date_time != '', year < 2015) %>%
  group_by(year, month) %>%
  summarise(count = n()) %>%
  group_by(month) %>%
  summarise(avg_sightings = mean(count)) %>%
  arrange(month)
monthly_sightings$month_name <- factor(month.abb, levels = month.abb)
ggplot(monthly_sightings, aes(month_name, avg_sightings)) +
  geom_bar(stat='identity') +
  labs(x='', y='', title='Promedio de avistamientos por mes')
```

**Promedio de avistamientos mensuales por estado**
Como vemos en los mapas, hay una gran diferencia entre el número de avistamientos totales y el per cápita. Al parecer la población afecta mucho los avistamientos. Es de esperarse que Nevada saliera con muchos avistamientos por persona, puesto que el desierto ahí es famoso por los UFOS.
```{r}
monthly_sightings <- ufo %>%
  filter(state != '', date_time != '', year < 2015) %>%
  group_by(state, year, month) %>%
  summarise(count = n()) %>%
  group_by(state) %>%
  summarise(avg_sightings = mean(count)) %>%
  arrange(avg_sightings) %>%
  mutate(state_ordered = factor(state, levels = state))
ggplot(monthly_sightings, aes(state_ordered, avg_sightings)) +
  geom_bar(stat='identity') +
  labs(x='', y='', title='Promedio de avistamientos por mes')

### Mapa
state_info <- state.x77 %>%
  as.data.frame %>%
  mutate(region = tolower(rownames(state.x77)))
names(state_info) <- tolower(names(state_info))
names(state_info)[c(4,6)] <- c('life_exp', 'hs_grad')
us_states <- map_data('state')
states_df <- us_states %>%
  left_join(state_abbr) %>%
  left_join(monthly_sightings) %>%
  left_join(state_info) %>%
  mutate(avg_sightings_per_capita = avg_sightings / population)
#head(states_df)
centers <- data.frame(state.center, state=state.abb)
names(centers) <- c('long','lat','state')
ggplot(states_df) +
  geom_polygon(aes(long,lat,group=group, fill=avg_sightings)) +
  geom_text(data=centers, aes(long, lat, label=state), color='grey') +
  labs(title = 'Avistamientos por estado')
ggplot(states_df) +
  geom_polygon(aes(long,lat,group=group, fill=avg_sightings_per_capita)) +
  geom_text(data=centers, aes(long, lat, label=state), color='grey') +
  labs(title = 'Avistamientos por estado por persona')
```

**Varianza de avistamientos mensuales por estado**
Hay que notar que en este caso los estados con mayor cantidad de avistamientos (debido a mayor población por ejemplo) tenderán a tener una mayor varianza. La segunda gráfica muestra los estados en el mismo orden, pero ahora la estadística es $\sigma/\mu$, la desviación normalizada con la media, en lugar de $\sigma$:
```{r}
monthly_state_var <- ufo %>%
  filter(state != '') %>%
  group_by(state, year, month) %>%
  summarise(count = n()) %>%
  group_by(state) %>%
  summarise(monthly_sd = sd(count),
            monthly_mean = mean(count),
            monthly_sd_mean_ratio = monthly_sd/monthly_mean,
            count = sum(count)) %>%
  arrange(desc(monthly_sd)) %>%
  mutate(state_ordered = factor(state, levels = state))
#monthly_state_var
ggplot(monthly_state_var, aes(state_ordered, monthly_sd)) +
  geom_bar(stat='identity')
ggplot(monthly_state_var, aes(state_ordered, monthly_sd_mean_ratio)) +
  geom_bar(stat='identity')
# ggplot(monthly_state_var, aes(count, monthly_sd_mean_ratio)) +
#   geom_point()
```

**Olas temporales**
En la escala logarítmica podemos ver con claridad que antes de 1940 casi no había avistamientos. En esa década los avistamientos empezaron a tener un crecimiento exponencial, con un bache en los 80s. El crecimiento fue retomado después y sigue la misma tendencia exponencial que antes.
```{r}
aux <- yearly_sightings %>%
  mutate(group = ifelse(year < 1940, '< 1940', '1940+'))
ggplot(aux, aes(year, count, color=group)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  geom_smooth(method = 'lm')
```

**Olas espaciales**
Para las olas espacio-temporales veremos la información agregada por década. Hasta 1950 juntamos la información por ser muy poca. Como vemos, aunque el lugar con más avistamientos cambia si hacemos el análisis per cápita o no, lo que es consistente es que hubo muchos avistamientos en la década del 2000. Habría que conseguir información poblacional más precisa para hacer este análisis más finamente.
```{r}
state_sightings_block <- ufo %>%
  filter(state != '') %>%
  mutate(block = cut(year, c(1800,seq(1950,2020,10)), dig.lab = 4)) %>%
  group_by(state, block) %>%
  summarise(sightings = n())
states_df_block <- fortify(us_states) %>%
  left_join(state_abbr) %>%
  left_join(state_sightings_block) %>%
  left_join(state_info) %>%
  mutate(sightings_per_capita = sightings / population) %>%
  filter(!is.na(block))
ggplot(states_df_block) +
  geom_polygon(aes(long,lat,group=group, fill=sightings)) +
  geom_text(data=centers, aes(long, lat, label=state), color='grey', size = 3) +
  facet_wrap(~ block) +
  labs(title = 'Avistamientos a lo largo del tiempo') +
  theme(legend.position='none')
ggplot(states_df_block) +
  geom_polygon(aes(long,lat,group=group, fill=sightings_per_capita)) +
  geom_text(data=centers, aes(long, lat, label=state), color='grey', size = 3) +
  facet_wrap(~ block) +
  labs(title = 'Avistamientos por persona (pob. 1975) a lo largo del tiempo') +
  theme(legend.position='none')
```

**Narrativas parecidas**
Para las narrativas parecidas consideraremos la longitud promedio de las descripciones largas. Queremos ver si hay alguna diferencia entre los diferentes estados en este sentido. Y de hecho vemos algunos datos interesantes. Por ejemplo, parece ser que en Dakota del Norte las descripciones son las más largas, mientras que las de Dakota del Sur se encuentran entre las más cortas. Al parecer en Kansas e Idaho tampoco describen mucho lo que ven. En el bloque sureste también hay descripciones más cortas. Esto podría deberse por ejemplo a temas religiosos.
```{r}
state_narratives <- ufo %>%
  filter(state != '') %>%
  mutate(long_desc_len = nchar(long_description)) %>%
  group_by(state) %>%
  summarise(mean_long_desc_len = mean(long_desc_len))
states_df_narr <- left_join(states_df, state_narratives)
head(states_df_narr)
ggplot(states_df_narr) +
  geom_polygon(aes(long,lat,group=group, fill=mean_long_desc_len)) +
  geom_text(data=centers, aes(long, lat, label=state), color='white') +
  labs(title = 'Longitud promedio de las descripciones largas') +
  theme(legend.position='none')
```


**Características sociales**
Para ver si las características sociales tienen algún efecto sobre la cantidad de avistamientos, graficaremos algunas de ellas contra el promedio mensual de avistamientos. En particular graficaremos la tasa de analfabetismo, la de asesinatos, la esperanza de vida y la tasa de graduación de la univarsidad. Para graficarlas juntas más fácilmente normalizamos cada una a su máximo. Interesantemente, no parece haber mucha relación entre estas variables sociales y los avistamientos.
```{r}
states_social <- states_df_narr %>%
  group_by(region, state, avg_sightings, population, income, illiteracy,
           life_exp, murder, hs_grad, frost, area, avg_sightings_per_capita,
           mean_long_desc_len) %>%
  summarise %>% ungroup %>%
  mutate(sight_per_illit = avg_sightings / (population * (illiteracy/100)))
# head(states_social)
states_social_plot <- rbind(
    data.frame(id='illiteracy', state= states_social$state,
          avg_sightings=states_social$avg_sightings, y=states_social$illiteracy),
    data.frame(id='murder', state= states_social$state, 
               avg_sightings=states_social$avg_sightings, y=states_social$murder),
    data.frame(id='life_exp', state= states_social$state,
               avg_sightings=states_social$avg_sightings, y=states_social$life_exp),
    data.frame(id='hs_grad', state= states_social$state,
               avg_sightings=states_social$avg_sightings, y=states_social$hs_grad)
  ) %>%
  filter(!is.na(y)) %>%
  group_by(id) %>%
  mutate(y = y/max(y)) # Normalizamos para comparar
# head(states_social_plot)
ggplot(states_social_plot, aes(avg_sightings, y)) +
  geom_text(aes(label=state)) +
  geom_smooth(method='loess') +
  facet_wrap(~ id)
```

**Modelo predictivo: Kriging sobre la cantidad de avistamientos**
Con el siguiente modelo intentaremos predecir espacialmente la cantidad de avistamientos que esperamos ver. Utilizaremos la información de 2014 con el fin de predecir en 2015. Sumaremos los avistamientos de un estado y supondremos que fueron vistos desde el punto medio del estado. Entonces los centros de los estados son puntos fijos en los que observamos una cantidad aleatoria de avistamientos. Haremos Krigging ordinario para tener una predicción continua de los avistamientos para 2015. Primero veamos un mapa con los avistamientos en el centro, denotados por el tamaño de los puntos:
```{r}
parcial <- ufo[-long_vars] %>% 
  filter(date_time >= '2014-01-01' & date_time <= '2014-12-31') %>%
  group_by(state) %>%
  summarise(avistamientos = n()) %>%
  left_join(centers) %>%
  filter(!is.na(long) & !is.na(lat)) %>%
  data.frame
ggplot(mapping=aes(long,lat)) +
  geom_polygon(data=states_df, aes(group=group), fill='grey') +
  geom_point(data=parcial, aes(size=avistamientos)) +
  theme_nothing(legend=TRUE)
```

Ahora graficamos el variograma empírico:

```{r}
library(geoR)
spat <- as.geodata(parcial, coords.col = 3:4, data.col = 2)
v_emp <- variog(spat, max.dist = 50)
v_emp_df <- data.frame(v_emp[c('u','v','n')])
names(v_emp_df) <- c('dist','gamma','np')
ggplot(v_emp_df, aes(dist, gamma)) +
  geom_point() +
  geom_line() +
  geom_text(aes(label=np), vjust=-0.5) +
  ylim(0,64000) +
  labs(title='Variograma empírico (los números son la población)',
       x = 'Distancia', y = 'Variograma (gamma)')
v_fit <- variofit(v_emp,  cov.model='exponential', ini.cov.pars = c(40000,20), nugget = T)
#v_fit
```

Y finalmente mostramos nuestras predicciones continuas para 2015 en la escala de color. Los puntos nuevamente representan 

```{r}
spat.grid <- expand.grid(long=seq(min(states_df$long),max(states_df$long), len = 200),
                         lat=seq(min(states_df$lat),max(states_df$lat), len = 200))
krig <- krige.conv(spat, locations=spat.grid, krige=krige.control(obj.model=v_fit))
krig_df <- data.frame(spat.grid, pred=krig$predict)
states_df2 <- states_df %>%
  dplyr::select(long, lat, group) 
library(sp)
s <- SpatialPoints(spat.grid)
polys <- list()
for(i in unique(states_df2$group)){
  polys[[i]] <- Polygon(states_df2[states_df2$group==i,c('long','lat')])
}
plist <- Polygons(polys, ID = 'group')
p <- SpatialPolygons(list(plist))

idx_in <- which(!is.na(sp::over(s,p)))

ggplot(mapping=aes(long,lat)) +
  geom_raster(data=krig_df[idx_in,], aes(fill=pred)) +
  geom_path(data=states_df2, aes(group=group)) +
  geom_text(data=parcial, aes(size=avistamientos, label=state)) +
  scale_fill_gradientn(colours = (brewer.pal(7,'YlOrRd'))) +
  scale_size_continuous(range = c(1,10)) +
  theme_nothing(legend = T)
```



GDELT
----------------------------------------------------------------------------

## 1. Obtención de los datos

Obtuvimos los datos de [GDELT](http://gdeltproject.org) en un proyecto anterior, utilizando _bash_ y _R_ para scrappear la página.

## 2. Limpieza

Para esta base también decidimos utilizar una estrategia ELT. Dado que la extracción ya había sido llevada a cabo, pasamos directamente a la carga. La base de eventos de GDELT pesa alrededor de 12GB cuando está comprimida. Cuando se descomprime pesa cerca de 100GB. Debido a eso, nos fue imposible descomprimirla antes de subirla. Lo que hicimos fue descomprimirla directamente al stdout y subirla directamente con `\COPY` a PostgreSQL. En esta ocasión la información era de mucho mejor calidad que en la base de UFO, así que la base sucia de subida sí tenía ya las columnas definitivas, pero en formato de texto. Dado que la base es enorme, tardó varias horas en descomprimirse y subirse, pero quedó en menos de una noche. Una forma de haber acelerado el proceso a costa de menor seguridad de los datos habría sido crear esta tabla como _unlogged_, aunque el comando `\COPY` no sufre tanto como `INSERT`.

Para crear la base limpia, creamos la tabla maestra y luego creamos la partición de acuerdo a los archivos originales. Lo que esto implica es que hay algunas tablas que están por año, otras por mes y otras por día, pero como las que corresponden a intervalos de tiempo más cortos son más nuevas, también tienen más información. Optamos por este esquema porque si no la partición consistiría de pocas tablas muy grandes, en especial las recientes. En este caso también generamos los _triggers_ apropiados para que la tabla se comportara como una sola. Y ya con la estructura construida, insertamos desde la tabla sucia pero poniendo los tipos, formatos y nulos apropiados. El proceso tardó alrededor de dos días con pausas, trabajando toda la noche y buena parte del día. Creemos que una buena parte se debe a que al momento de hacer este paso no habíamos optimizado la configuración del PostgreSQL.


## 3. Optimización del PostgreSQL

Para que los queries terminaran en tiempo finito, tuvimos que configurar correctamente el PostgreSQL. A continuación enlistamos lo que cambiamos:

* Redujimos el número máximo de conexiones (`max_connections`) de 100 a 5. En realidad no las usamos, pero ocasionalmente querremos abrir más de una sesión.
* Aumentamos el caché para tener datos (`shared_buffers`) de 128MB a 1GB
* Duplicamos `temp_buffers` de 8MB a 16MB
* Aumentamos la memoria para ordenar y hacer _joins_ (`work_mem`) de 4MB a 1GB. Este parámetro nos ayudó _muchísimo_. Con la configuración inicial ordenar casi cualquier tabla tomaba demasiado tiempo.
* 







## 3. Análisis descriptivo


Hubo que optimizar PostgreSQL, hacer queries inteligentes, etc.
