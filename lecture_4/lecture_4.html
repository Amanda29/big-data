<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 4</title>
<meta name="author" content="(Adolfo De Unánue)"/>

<link rel="stylesheet" href="http://cdn.jsdelivr.net/reveal.js/2.5.0/css/reveal.css"/>
<link rel="stylesheet" href="http://cdn.jsdelivr.net/reveal.js/2.5.0/css/theme/night.css" id="theme"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'http://cdn.jsdelivr.net/reveal.js/2.5.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<meta name="description" content="Big data - Lecture 6.">
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1>Lecture 4</h1>
<h2>Adolfo De Unánue</h2>
<h2><a href="mailto:adolfo.deunanue@itam.mx">adolfo.deunanue@itam.mx</a></h2>
<h2></h2>
</section>

<section>
<section id="slide-sec-1">
<h2 id="sec-1">Docker</h2>
<div class="outline-text-2" id="text-1">
</div></section>
<section id="slide-sec-1-1">
<h3 id="sec-1-1">Obtener la imagen</h3>
<pre class="example">
docker pull nanounanue/docker-hadoop
</pre>

</section>
<section id="slide-sec-1-2">
<h3 id="sec-1-2">Ejecutar un contenedor</h3>
<pre class="example">
docker run -ti --rm \
  -e "AUTHORIZED_SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub)" \
  -v /home/nano/tmp/docker-hadoop-data/:/home/hduser/hdfs-data/ \
  -v /home/nano/tmp/docker-hadoop-logs/:/srv/hadoop/logs/ \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888
  -p 8000:8000 \
nanounanue/docker-hadoop /bin/zsh
</pre>

</section>
<section id="slide-sec-1-3">
<h3 id="sec-1-3">Contenedor con Hadoop</h3>
<pre class="example">
docker run -ti --name hadoop_pseudodistribuido \
  -e "AUTHORIZED_SSH_PUBLIC_KEY=$(cat ~/.ssh/id_rsa.pub)" \
  -v /home/nano/tmp/docker-hadoop-data/:/home/hduser/hdfs-data/ \
  -v /home/nano/tmp/docker-hadoop-logs/:/srv/hadoop/logs/ \
  -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
  -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
  -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
  -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
  -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
  -p 8000:8000 \
nanounanue/docker-hadoop
</pre>

</section>
<section id="slide-sec-1-4">
<h3 id="sec-1-4">Conectarse al contenedor</h3>
<pre class="example">
ssh hduser@localhost -p 2122
</pre>

</section>
<section id="slide-sec-1-5">
<h3 id="sec-1-5">Navegador Web</h3>
<ul>
<li><a href="http://127.0.0.1:50090">Consola de Yarn</a></li>

<li><a href="http://127.0.0.1:50070">Consola de HDFS</a></li>

<li><a href="http://0.0.0.0:8000">HUE - Hadoop User Experience</a></li>

</ul>

</section>
</section>
<section>
<section id="slide-sec-2">
<h2 id="sec-2">Apache Hadoop</h2>
<div class="outline-text-2" id="text-2">
</div></section>
<section id="slide-sec-2-1">
<h3 id="sec-2-1">¿Por qué?</h3>
<ul>
<li>Aunque la capacidad de los discos ha aumentado considerablemente, la velocidad de los mismos no lo ha hecho igual.
<ul>
<li>Los discos actuales de <code>1 Tb</code>, tardan en leerse completos a <code>100 Mb/s</code> cerca de dos a tres horas.</li>
<li>Podemos <i>paralelizar</i> las fuentes en varios discos.
<ul>
<li>Para leerla simultáneamente</li>

</ul></li>
<li>Con varios discos, la <b><b>probabilidad de falla</b></b> aumenta.</li>

</ul></li>
<li>Otro problema es la distribución ¿Cómo combinas varios <code>file systems</code>?</li>

</ul>

</section>
<section id="slide-sec-2-2">
<h3 id="sec-2-2">¿Qué es?</h3>
<ul>
<li>Sistema confiable (<i>realiable</i>) de almacenamiento compartido y de análisis.
<ul>
<li><b>Almacenamiento</b>: HDFS</li>
<li><b>Análisis</b>: MapReduce</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-3">
<h3 id="sec-2-3">¿Cómo?</h3>
<ul>
<li><code>MapReduce</code> es un sistema de procesamiento <i>batch</i>
<ul>
<li>Permite correr <i>queries</i> contra <b><b>toda</b></b> tu base de datos</li>
<li>Pero el resultado puede tardar minutos, horas, etc&#x2026;</li>
<li>No permite tener a un humano sentado ahí para retroalimentar.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-4">
<h3 id="sec-2-4">¿Cómo?</h3>
<ul>
<li>Ahora, gracias a <code>YARN</code> (ver más adelante) tenemos diferentes tipos de procesamiento:
<ul>
<li><i>SQL Interactivo</i>: <code>Impala</code>, <code>Hive</code>, <code>Tez</code>.</li>
<li><i>Iterativos</i>: <code>Spark</code>.</li>
<li><i>Procesamiento de flujos</i>: <code>Storm</code>, <code>Spark Streaming</code>.</li>
<li><i>Búsquedas</i>: <code>Solr</code>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-5">
<h3 id="sec-2-5">¿Por qué no otros sistemas?</h3>
<ul>
<li>¿Por qué no usar un <code>PostgreSQL</code> con muchos discos, muy <i>pimpeado</i>?
<ul>
<li>El problema viene del tiempo que toma mover la cabeza del disco a otro lugar del disco para leer o escribir datos (<i>seek time</i>).
<ul>
<li>¿Cuál es la <i>latencia</i> de la operación?</li>

</ul></li>

</ul></li>

<li>¿Por qué no <i>Grid</i>?
<ul>
<li>Por ejemplo, cosas  de <code>HPC</code> que usan <code>MPI</code>.
<ul>
<li>Son intensivos en <b><b>CPU</b></b>.</li>

</ul></li>
<li>Pero si hay que mover cientos de gigabytes, la transferencia de datos se vuelve un problema.
<ul>
<li>Basicamente, en que <code>Hadoop</code> opera con <i>data locality</i>.</li>

</ul></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-2-6">
<h3 id="sec-2-6">Componentes de Apache Hadoop</h3>
<ul>
<li><b>MapReduce</b> Modelo de procesamiento <i>batch</i> de datos distribuido y paralelo.</li>
<li><b>HDFS</b> Sistema de archivos (<i>file system</i>) distribuido.</li>
<li><b>Pig</b> Capa de abstracción encima de <code>MapReduce</code>. Utiliza <i>Pig Latin</i> un lenguaje de flujo de datos
<ul>
<li>Como <code>dplyr</code></li>

</ul></li>
<li><b>Hive</b> (Hadoop InteractiVE) Es un lenguaje parecido al <code>SQL</code>: <code>HQL</code>, para ejecutar <i>queries</i> sobre el <code>HDFS</code>.</li>
<li><b>HBase</b> Base de datos distribuida orientada a columnas.
<ul>
<li>Depende de <code>Zookeeper</code>.</li>

</ul></li>
<li><b>Impala</b> Lenguaje Interactivo parecido al <code>SQL</code>, pero mucho más rápido de <code>HIVE</code> debido a su arquitectura <b>MPP</b>.</li>

</ul>

</section>
<section id="slide-sec-2-7">
<h3 id="sec-2-7">Componentes de Apache Hadoop</h3>
<ul>
<li><b>Zookeeper</b> Sistema distribuido de coordinación.</li>
<li><b>Sqoop</b> Herramienta para mover datos entre <code>RDBM</code> y <code>HDFS</code>.</li>
<li><b>Flume</b> Servicio para recolectar, agregar y mover grandes cantidades de datos entre máquinas individuales y el <code>HDFS</code>.</li>
<li><b>Oozie</b> Sistema de <i>workflow</i>, se usa para coordinar varios <i>jobs</i> de <b>MapReduce</b>.</li>
<li><b>Mahout</b> Biblioteca de <i>Machine Learning</i>.
<ul>
<li>Ver la carpeta <code>docs</code>.</li>

</ul></li>
<li><b>Ambari</b> Simplifica el aprovisionamiento, gestión y <i>monitoreo</i> de un <i>cluster</i> de Hadoop.</li>
<li><b>Avro</b> Formato de serialización y de persistencia de datos.</li>
<li>Entre otros&#x2026;</li>

</ul>



</section>
</section>
<section>
<section id="slide-sec-3">
<h2 id="sec-3">HDFS : Hadoop File System</h2>
<div class="outline-text-2" id="text-3">
</div></section>
<section id="slide-sec-3-1">
<h3 id="sec-3-1">HDFS</h3>
<ul>
<li>Sistema de almacenamiento distribuido.
<ul>
<li><i>Namenode</i> <code>-&gt;</code> Master</li>
<li><i>Datanode</i> <code>-&gt;</code>  Slaves</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-2">
<h3 id="sec-3-2">Ventajas</h3>
<ul>
<li>Archivos muy grandes</li>
<li><i>write once, read many times</i>.</li>
<li>Hardware <span class="underline">normal</span></li>

</ul>

</section>
<section id="slide-sec-3-3">
<h3 id="sec-3-3">Desventajas</h3>
<ul>
<li>Acceso a los datos de baja latencia.</li>
<li>Muchos archivos pequeños.</li>
<li>Muchas escrituras, modificaciones</li>

</ul>

</section>
<section id="slide-sec-3-4">
<h3 id="sec-3-4">Tamaño del bloque</h3>
<ul>
<li>Cada <i>file system</i> define un tamaño de bloque, el cual es la cantidad mínima de datos que puede escribir o leer.
<ul>
<li>Típicamente son de <code>kb</code>.</li>

</ul></li>
<li>En <code>HDFS</code>, el bloque es de <code>128 Mb</code> por <i>default</i>.
<ul>
<li>Es el concepto fundamental, no el archivo.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-3-5">
<h3 id="sec-3-5"><i>Namenode</i></h3>
<ul>
<li>Gestiona el <i>filesystem</i>
<ul>
<li>Mantiene el árbol del <i>filesystem</i>.</li>
<li>Mantiene los <code>metadatos</code> de todos los archivos y carpetas del árbol.</li>
<li>Esta información se guarda en disco en dos archivos:
<ul>
<li><code>namespace image</code></li>
<li><code>edit log</code></li>

</ul></li>

</ul></li>
<li>Indica a los <i>datanodes</i> realizar tareas de bajo nivel de <code>I/O</code>.</li>
<li><i>Book Keeper</i>
<ul>
<li>División de archivos en bloques (¿Cómo?)</li>
<li>En qué <i>datanode</i> (¿Quién?)</li>
<li>Monitorea.</li>

</ul></li>
<li>Uso intensivo de <code>RAM</code> y de <code>I/O</code>.</li>
<li>Si se <i>cae</i> el <code>HDFS</code> no puede ser usado
<ul>
<li>Hasta la versión <code>1.x</code> el <i>single point of failure</i>, en Hadoop 2 se incorporó la característica de <i>HIgh Availability</i>.</li>
<li>Su caída puede causar la pérdida total de los datos.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-6">
<h3 id="sec-3-6"><i>Namenode</i></h3>
<ul>
<li>Hadoop proveé de dos formas de aliviar esta situación:
<ul>
<li>Respaldos: Se puede configurar al <i>namenode</i> para que escriba su estado a varios <i>filesystems</i>.</li>
<li><i>Secondary Namenode</i></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-3-7">
<h3 id="sec-3-7"><i>Namenode</i></h3>

<div class="figure">
<p><img src="./imagenes/Selección_004.png" alt="Selección_004.png" />
</p>
</div>


</section>
<section id="slide-sec-3-8">
<h3 id="sec-3-8"><i>Datanode</i></h3>
<ul>
<li>Lee y escribe los <code>HDFS</code> <i>blocks</i> y los convierte en archivos del <b>FS</b> local.</li>
<li>Se comunica con otros <i>datanodes</i> para la replicación de los datos.</li>
<li>Pueden realizar <i>caching</i> de bloques.</li>

</ul>

</section>
<section id="slide-sec-3-9">
<h3 id="sec-3-9"><i>Datanode</i></h3>

<div class="figure">
<p><img src="./imagenes/Selección_005.png" alt="Selección_005.png" />
</p>
</div>

</section>
<section id="slide-sec-3-10">
<h3 id="sec-3-10"><i>Secondary Name Node</i></h3>
<ul>
<li>Como el <i>namenode</i> sólo hay uno por <i>cluster</i>.</li>
<li>No es un <i>namenode</i>.</li>
<li>Evita que el <code>edit log</code> crezca mucho.</li>
<li>No recibe ni guarda cambios en tiempo real del <code>HDFS</code>.
<ul>
<li>Va atrás del <i>namenode</i>.</li>

</ul></li>
<li>Sólo toma <i>snapshots</i> de la metadata.</li>

</ul>


</section>
<section id="slide-sec-3-11">
<h3 id="sec-3-11">Línea de comandos</h3>
<ul>
<li>Hay muchas maneras de conectarse y usar el <code>HDFS</code>. La línea de comandos es una de ellas.
<ul>
<li>Y espero que ya sepan que es de las más útiles y eficientes.</li>

</ul></li>

<li>Ayuda: <code>hadoop fs -help</code></li>

</ul>

</section>
<section id="slide-sec-3-12">
<h3 id="sec-3-12">Línea de comandos</h3>
<pre class="example">
hadoop fs -cmd &lt;args&gt;
hadoop fs -ls
hadoop fs -mkdir
hadoop fs -copyFromLocal
hadoop fs -copyToLocal
hadoop fs -put archivo archivo_hdfs
hadoop fs -get archivo_hdfs
hadoop fs -cat archivo_hdfs
hadoop fs -cat archivo_hdfs head
hadoop fs -tail archivo_hdfs
hadoop fs -rm archivo_hdfs
</pre>


</section>
<section id="slide-sec-3-13">
<h3 id="sec-3-13">Modo Pseudodistribuido</h3>
<ul>
<li>Crea una imagen sin Hadoop corriendo, vamos a explicar que significa <i>pseudodistribuido</i>.</li>

</ul>

</section>
<section id="slide-sec-3-14">
<h3 id="sec-3-14">Ejercicio</h3>
<ul>
<li>Crea una imagen con Hadoop corriendo.</li>
<li>Conéctate con el usuario <code>hduser</code>.</li>
<li>Verifique que <code>alias</code> tiene definido el usuario <code>hduser</code>.
<ul>
<li>Usa el comando <code>alias</code>.</li>

</ul></li>
<li>Crear una carpeta <code>ufo</code> en el <code>HDFS</code> y suba  los archivos de <code>ufo</code> a la carpeta recién creada.
<ul>
<li>Descomprime los archivos antes de subirlos</li>
<li>Crea un <i>script</i> para esta tarea, llámalo <code>ufo_hdfs.sh</code>.</li>

</ul></li>
<li>Crear una carpeta <code>gdelt</code> en el <code>HDFS</code> y suba los archivos de <code>gdelt</code> a esta carpeta.
<ul>
<li>Descomprime los archivos antes de subirlos</li>
<li>Crea un <i>script</i> para esta tarea, llámalo <code>gdelt_hdfs.sh</code>.</li>

</ul></li>
<li>Muestra las carpetas en la línea de comandos.
<ul>
<li>Modifica los usuarios y permisos del HDFS ¿Cómo crees que se haga?</li>

</ul></li>
<li>Muestra las carpetas en la vista web.</li>

</ul>

</section>
</section>
<section>
<section id="slide-sec-4">
<h2 id="sec-4">YARN</h2>
<div class="outline-text-2" id="text-4">
</div></section>
<section id="slide-sec-4-1">
<h3 id="sec-4-1">YARN</h3>
<ul>
<li>La infraestructura de Hadoop <code>0.x</code> y <code>1.x</code> era monolítica, por eso fue rediseñada.</li>
<li><code>YARN</code>: <i>Yet Another Resource Negotiator</i>.</li>
<li>La gestión de recursos es extraída de los paquetes de <code>MapReduce</code> para que puedan ser utilizadas por otros componentes.</li>
<li>Aportaciones
<ul>
<li>Escalabilidad.</li>
<li>Compatibilidad con <code>MapReduce</code>.</li>
<li>Mejoras en la gestión del <i>cluster</i>.</li>
<li>Soporte para otros modelos de programación (además de <code>MapReduce</code>).
<ul>
<li><i>Graph processing</i></li>
<li><i>Message Passing Interface</i> (<b>MPI</b>).</li>
<li>Soporte para procesamiento <i>real-time</i> o <i>near real-time</i>.
<ul>
<li><code>MapReduce</code> es <i>batch-oriented</i>.</li>

</ul></li>

</ul></li>
<li>Agilidad.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-2">
<h3 id="sec-4-2">YARN</h3>
<ul>
<li>Se dividieron las dos responsabilidades del <i>JobTracker</i>:
<ul>
<li>Gestión de recursos (<i>Resource Management</i>)</li>
<li>Asignación y vigilancia de trabajos (<i>Job scheduling-monitoring</i>)</li>

</ul></li>

<li>La idea es tener un <i>ResourceManager</i> global y un <i>NodeManager</i> por
nodo esclavo, los cuales forman un sistema para la administración de
aplicaciones distribuidas.</li>

<li>El <i>ResourceManager</i> tiene dos componentes principales:
<ul>
<li><i>Scheduler</i>: Asigna los recursos para las aplicaciones (<i>pluggeable</i>).</li>
<li><i>Application Manager</i>: Responsable de aceptar las solicitudes de
trabajos, negociando al principio para ejecutar el <i>Application
Master</i> específico y provee un servicio de reinicio, por si el
<i>Application Master</i> falla.</li>

</ul></li>

<li>En cada nodo:

<ul>
<li>El <i>Application Master</i>: Negocia sus recursos con el <i>Scheduler</i>,</li>

</ul>
<p>
monitorea sus avances y reporta su estatus.
</p>

<ul>
<li>El <i>NodeManager</i> es el responsable de los contenedores,
monitorear el uso de recursos y reportar todo al
<i>ResourceManager</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-4-3">
<h3 id="sec-4-3">Arquitectura MapReduce Hadoop 1.x</h3>

<div class="figure">
<p><img src="./imagenes/MRArch.png" alt="MRArch.png" />
</p>
</div>

</section>
<section id="slide-sec-4-4">
<h3 id="sec-4-4">Arquitectura Hadoop 2.x</h3>

<div class="figure">
<p><img src="./imagenes/Selección_003.png" alt="Selección_003.png" />
</p>
</div>


</section>
<section id="slide-sec-4-5">
<h3 id="sec-4-5">Cambios 1.x -&gt; 2.x</h3>

<div class="figure">
<p><img src="./imagenes/yarn.png" alt="yarn.png" />
</p>
</div>


</section>
<section id="slide-sec-4-6">
<h3 id="sec-4-6">Multiparadigma en Hadoop 2.x</h3>

<div class="figure">
<p><img src="./imagenes/YARN.png" alt="YARN.png" />
</p>
</div>


</section>
</section>
<section>
<section id="slide-sec-5">
<h2 id="sec-5">Procesamiento</h2>
<div class="outline-text-2" id="text-5">
</div></section>
<section id="slide-sec-5-1">
<h3 id="sec-5-1">Tipos</h3>
<ul>
<li>MapReduce</li>
<li>Spark</li>
<li>Impala</li>

</ul>

</section>
<section id="slide-sec-5-2">
<h3 id="sec-5-2">MapReduce en Hadoop</h3>
<ul>
<li>Principal <i>framework</i> de ejecución de <code>Apache Hadoop</code>.</li>
<li>Inspirado en las operaciones <b>MAP</b> y <b>REDUCE</b> de los lenguajes funcionales.</li>
<li>Modelo de programación para proceso de datos distribuido  y paralelo.</li>
<li>Divide las tareas (<i>jobs</i>) en fases de <i>mapeo</i> y fases de <i>reducción</i>.</li>
<li>Los desarrolladores crean tareas <i>MapReduce</i> para Hadoop usando datos guardados en el <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-5-3">
<h3 id="sec-5-3">MapReduce: Ventajas</h3>
<ul>
<li><i>Fault-tolerant</i>.</li>
<li>Esconde los detalles de implementación a los programadores.</li>
<li>Escala con el tamaño de los datos.</li>

</ul>


</section>
<section id="slide-sec-5-4">
<h3 id="sec-5-4">MapReduce</h3>
<ul>
<li>Dos fases de procesamiento:
<ul>
<li><i>key-value</i> como Input y Output</li>
<li>El programador especifica:
<ul>
<li>Tipos de <i>key-value</i></li>
<li>Funciones: <code>MAP</code> y <code>REDUCE</code>.</li>

</ul></li>

</ul></li>

</ul>


</section>
<section id="slide-sec-5-5">
<h3 id="sec-5-5">Una pequeña regresión&#x2026;</h3>

</section>
<section id="slide-sec-5-6">
<h3 id="sec-5-6">map-reduce: Matemáticamente</h3>
<pre class="example">
map: (k1, v1) -&gt; list(k2, v2)
</pre>

<ul>
<li><code>map</code> Mapea (aplica una función <i>f</i>) un conjunto de entrada de pares <i>key-value</i> a otro conjunto intermedio de <i>key-values</i></li>

</ul>


</section>
<section id="slide-sec-5-7">
<h3 id="sec-5-7">map-reduce: Matemáticamente</h3>
<pre class="example">
reduce: (k2, list(v2)) -&gt; list(k3, v3)
</pre>

<ul>
<li><code>reduce</code>  Aplica una función <i>g</i> a todos los valores (<i>values</i>) asociados a una llave (<i>key</i>) y acumula el resultado. Emite pares de <i>key-values</i>.</li>

</ul>

</section>
<section id="slide-sec-5-8">
<h3 id="sec-5-8">Python <code>map</code></h3>
<div class="org-src-container">

<pre  class="src src-python"># Equivalente en for-loop

items = [1,2,3,4,5]
cuadrados = []
for x in items:
    cuadrados.append(x**2)

print cuadrados
</pre>
</div>


<div class="org-src-container">

<pre  class="src src-python"># Usando la funcion map(function, sequence)

items = [1,2,3,4,5]

print list(map((lambda x: x**2), items))
</pre>
</div>


</section>
<section id="slide-sec-5-9">
<h3 id="sec-5-9">Python <code>reduce</code></h3>
<div class="org-src-container">

<pre  class="src src-python"># Equivalente en for-loop
L = [1,2,3,4]
result = L[0]
for x in L[1:]:
    result = result*x

print result
</pre>
</div>

<div class="org-src-container">

<pre  class="src src-python"># Usando la funcion reduce(funcion, secuencia)
print reduce((lambda x,y: x*y), [1,2,3,4])
</pre>
</div>

</section>
<section id="slide-sec-5-10">
<h3 id="sec-5-10">Python <code>map</code> y <code>reduce</code></h3>
<div class="org-src-container">

<pre  class="src src-python">a = range(1, 4)
b = range(4, 9)
c = range(9, 15)
print "a -&gt;  %s, b -&gt; %s , c -&gt; %s" % (a, b, c)

L1 = map(lambda x:len(x), [a,b,c])
print "L1 -&gt; %s" % L1

L2 = reduce(lambda x, y: x+y, L1)
print "L2 -&gt; %s" % L2
</pre>
</div>



</section>
<section id="slide-sec-5-11">
<h3 id="sec-5-11">MapReduce y map-reduce</h3>
<ul>
<li>Básicamente es lo mismo, pero&#x2026;</li>
<li><code>map</code>, <code>reduce</code> (entre otras) son parte de lenguajes funcionales.</li>
<li><code>MapReduce</code> es la aplicación de esta idea aplicada a problemas <i>vergonzosamente</i> <i>paralelos</i>.
<ul>
<li>Ver la carpeta <code>docs</code> para el artículo de <b>Google</b> sobre <code>MapReduce</code>.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-5-12">
<h3 id="sec-5-12">GNU Parallel</h3>
<div class="org-src-container">

<pre  class="src src-sh">find ./data/books -type f | parallel -j0 egrep -i  '\[\[:digit:\]\]' {} | awk '{s+=$1} END {print s}'
</pre>
</div>


<ul>
<li><b>¿Puedes identificar las partes <code>map</code> y <code>reduce</code>?</b></li>
<li>Esto ya es un <code>MapReduce</code>.</li>

</ul>


</section>
<section id="slide-sec-5-13">
<h3 id="sec-5-13">MapReduce en Hadoop</h3>
<ul>
<li>A nivel programático:
<ul>
<li><i>Data</i> de entrada</li>
<li>Programa MapReduce</li>
<li>Configuración</li>
<li>Subtareas: <code>map</code> y <code>reduce</code></li>

</ul></li>

</ul>


</section>
<section id="slide-sec-5-14">
<h3 id="sec-5-14">MapReduce: <i>Mapper</i></h3>
<ul>
<li>Hadoop divide la entrade de datos al <i>job</i> MapReduce en pedazos de tamaño fijo llamados <i>input splits</i>.</li>
<li>Hadoop crea una tarea <code>map</code> para cada <i>input split</i>.</li>
<li><code>map</code> escribe al <i>file system</i> local.
<ul>
<li>Si el <code>reducer</code> tiene éxito se borra la salida del <i>mapper</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-5-15">
<h3 id="sec-5-15">Map only</h3>

<div class="figure">
<p><img src="./imagenes/map_only.png" alt="map_only.png" />
</p>
</div>


</section>
<section id="slide-sec-5-16">
<h3 id="sec-5-16">MapReduce: <i>Reducer</i></h3>
<ul>
<li>La entrada es la salida de (posiblemente) todos los <i>mappers</i>.</li>
<li>Estas se transmiten vía red al nodo donde corre el <i>reducer</i>.</li>
<li>La salida se guarda en el <code>HDFS</code>.</li>

</ul>

</section>
<section id="slide-sec-5-17">
<h3 id="sec-5-17">Map, One reduce</h3>

<div class="figure">
<p><img src="./imagenes/map_one_reduce.png" alt="map_one_reduce.png" />
</p>
</div>

</section>
<section id="slide-sec-5-18">
<h3 id="sec-5-18">MapReduce</h3>

<div class="figure">
<p><img src="./imagenes/map_reduce.png" alt="map_reduce.png" />
</p>
</div>


</section>
<section id="slide-sec-5-19">
<h3 id="sec-5-19">MapReduce: <i>Combiner</i></h3>
<ul>
<li>Es una medida de optimización.</li>
<li>Es para ahorrar ancho de banda.</li>
<li>Una especie de <i>reducer</i> local.</li>
<li>No es parte (estrictamente) del MapReduce
<ul>
<li>Por eso no lo había mencionado.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-5-20">
<h3 id="sec-5-20">Ejercicio</h3>
<ul>
<li>Diseñe el <b><b>MapReduce</b></b> para lo siguiente:
<ul>
<li>Encontrar el máximo de un conjunto de datos.</li>
<li>Encontrar el promedio y desviación estándar de unos datos.</li>
<li>Encontrar el top 10 de una cantidad.</li>
<li>Contar por grupo</li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-sec-6">
<h2 id="sec-6">Apache Haoop 2.x: YARN</h2>



</section>
</section>
<section>
<section id="slide-sec-7">
<h2 id="sec-7">Hello World!: Word count</h2>
<div class="outline-text-2" id="text-7">
</div></section>
<section id="slide-sec-7-1">
<h3 id="sec-7-1">Word count</h3>
<ul>
<li>Es el ejemplo <i>Hola Mundo</i> de Apache Hadoop.</li>
<li>No sólo eso, es el ejemplo que se utiliza en el trabajo seminal
<ul>
<li><b>MapReduce: Simplified Data Processing on Large Clusters</b> <i>(2006)</i>.</li>
<li>En la carpeta <code>docs</code> como ya había dicho.</li>

</ul></li>
<li>Solamente 1 <code>Map</code> y 1 <code>Reduce</code>.</li>

</ul>


</section>
<section id="slide-sec-7-2">
<h3 id="sec-7-2">Word count</h3>
<ul>
<li><b>mapper</b>
<ul>
<li><code>k1</code> -&gt; nombre de archivo</li>
<li><code>v1</code> -&gt; texto del archivo</li>
<li><code>k2</code> -&gt; palabra</li>
<li><code>v2</code> -&gt; "1"</li>

</ul></li>

<li><b>reducer</b>
<ul>
<li><code>k2</code> -&gt; palabra</li>
<li>list(v2) -&gt; (1,1,1,1,1,1,&#x2026;, 1)</li>

</ul>
<p>
Suma los "1" y produce una lista de
</p>

<ul>
<li>k3 -&gt; palabra</li>
<li>v3 -&gt; suma</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-7-3">
<h3 id="sec-7-3">Word count</h3>

<div class="figure">
<p><img src="./imagenes/word_count.png" alt="word_count.png" />
</p>
</div>

</section>
<section id="slide-sec-7-4">
<h3 id="sec-7-4">Pseudocódigo</h3>
<pre class="example">
map (String key, String value)
   for each word w in value
      Emit(w, 1)

reduce (String key, Iterator values)
   int wordcount = 0
   for each v in values
      wordcount += v
      Emit(key, wordcount)
</pre>

</section>
<section id="slide-sec-7-5">
<h3 id="sec-7-5">Mockup</h3>
<ul>
<li>Ver los archivos <code>word_count.py</code> y <code>mapreduce.py</code> en la carpeta <code>mock</code>.</li>

</ul>

<pre class="example">
chmod +x word_count.py
python word_count.py
</pre>

<ul>
<li>Este es un ejemplo de mentiritas, no usa Apache Hadoop.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-8">
<h2 id="sec-8">Pig</h2>
<div class="outline-text-2" id="text-8">
</div></section>
<section id="slide-sec-8-1">
<h3 id="sec-8-1">Pig</h3>
<ul>
<li>Proyecto de Apache</li>
<li>Abstracción encima de Hadoop
<ul>
<li><i>Pig Latin</i> compila a <code>MapReduce</code></li>
<li>En cierta forma <i>Pig Latin</i> es para analistas, <i>data scientist</i> y estadísticos.</li>
<li><code>MapReduce</code>  es para programadores (aunque los <i>data scientist</i> deberían de poder hacerlo también)</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-2">
<h3 id="sec-8-2">Pig</h3>
<ul>
<li>Pig es un <i>data flow programming language</i></li>
<li>Es decir,
<ul>
<li>Ejecuta paso a paso</li>
<li>Cada paso es una transformación de datos</li>

</ul></li>
<li>En cambio <code>SQL</code> es un conjunto de <i>constraints</i> que en conjunto definen el resultado buscado.</li>

</ul>

</section>
<section id="slide-sec-8-3">
<h3 id="sec-8-3">Pig</h3>
<ul>
<li>¿Qué cosas puede hacer?
<ul>
<li><code>joins</code></li>
<li><code>sorts</code></li>
<li><code>filters</code></li>
<li><code>group by</code></li>
<li><i>User defined functions</i> <code>UDF</code>'s</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-4">
<h3 id="sec-8-4">Pig</h3>
<ul>
<li>¿Qué cosas <b>puedo</b> hacer?

<ul>
<li><code>ETLs</code>
<ul>
<li>Limpiar.</li>
<li><i>Joins</i> gigantes.</li>

</ul></li>

<li>Búsqueda en <i>Raw</i>.</li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-5">
<h3 id="sec-8-5">Pig</h3>
<ul>
<li>Componentes
<ul>
<li><i>Pig Latin</i></li>
<li><code>Grunt</code>
<ul>
<li>Local</li>
<li>MapReduce</li>

</ul></li>
<li><code>Pig compiler</code></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-8-6">
<h3 id="sec-8-6">Pig</h3>
<ul>
<li>Es posible ejecutar también <i>scripts</i> de <i>Pig Latin</i> (terminación <code>.pig</code>) sin entrar a <code>grunt</code>.</li>

</ul>

<pre class="example">
pig script_file.pig

# Si quieren pasar parámetros
pig -p var=bla/bla var2=bla/bla/bla script_file.pig
</pre>

<ul>
<li>Y usarse desde programas en <code>Java</code> con la clase <code>PigServer</code>.
<ul>
<li>Como una especie de <code>JDBC</code>, pero para <i>Pig Latin</i>.</li>

</ul></li>

</ul>


</section>
<section id="slide-sec-8-7">
<h3 id="sec-8-7">Pig: <i>Building blocks</i></h3>
<p>
Fields
</p>
<pre class="example">
'Adolfo'
</pre>

<p>
Tuplas
</p>
<pre class="example">
('Adolfo', 3, 8.17, 23)
</pre>

<p>
<i>Bags</i>
</p>
<pre class="example">
{('Adolfo', 3, 8.17, 23), ('Paty', 3.14, 9, 'A')}
</pre>

</section>
<section id="slide-sec-8-8">
<h3 id="sec-8-8">Ejercicio</h3>
<ol>
<li>Crear una carpeta <code>rita</code> en el <code>HDFS</code>.</li>
<li>Agregar los siguientes archivos:
<ul>
<li><code>airports.csv</code></li>
<li><code>plane_data.csv</code></li>
<li><code>carriers.csv</code></li>

</ul></li>
<li>Ejecutar <code>grunt</code>.</li>

</ol>

<pre class="example">
# Pig latin puede ejecutar comandos del hdfs
cat rita/airports
# Especificando el separador (,) y el esquema (no es necesario)
airports = load 'rita/airports' using PigStorage(',') as (iata:chararray, ..., latitude:float, ...);
# Hasta este momento se ejecuta todo...
dump airports;
# El comando store guarda al HDFS y también ejecuta todo.
</pre>

</section>
<section id="slide-sec-8-9">
<h3 id="sec-8-9">Ejercicio</h3>
<pre class="example">
a_imprimir = limit airports 5;
por_estado = group airports by state;
describe por_estado;
explain por_estado;
illustrate por_estado;
# itera sobre cada elemento del bag
conteo = foreach por_estado generate group count_star(airports);
ordenados = order conteo by $1 desc;
top_five = limit ordenado 5;
unicos = distinct conteos;
muestreo = sample por_estado 0.1;
filtrados = filter conteos by substring(group, 0, 2) == 'W';
mayores = filter conteos by $1 &gt; 50;
</pre>

</section>
<section id="slide-sec-8-10">
<h3 id="sec-8-10">Ejercicio: Trucos del <code>foreach</code></h3>
<pre class="example">
# Proyectar
foreach airports generate iata, airport, country;

# Expresiones posicionales
# $1 -&gt; iata
# $3 -&gt; city
# $5 -&gt; country

# Rangos
# ..country, iata..country, latitude..

# Tokenizar
tokens = foreach lineas generate tokenize(linea);
# Cada fila obtenida es un bag de palabras.
</pre>

</section>
<section id="slide-sec-8-11">
<h3 id="sec-8-11">Pig: JOINS</h3>
<ol>
<li>Cargamos fuente 1</li>
<li>Cargamos fuente 2</li>
<li>Unimos las fuentes (<i>bags</i>) mediante una llave</li>
<li>Súper simple</li>

</ol>

<p>
Pig soporta <i>inner joins</i> (valor por omisión), <i>left outer joins</i> (y
<i>right</i> también) y <i>full outer</i> joins.
</p>


<pre class="example">
fuentes_unidas = join fuente1 by (keys) [left|right|full outer] fuente2 by (keys);
</pre>

<p>
Además <code>Pig</code> soporta <code>cogroup</code> además de los <code>joins</code> (el <code>cogroup</code>
preserva la estructura de las fuentes y crea tuplas por cada llave)
</p>

<pre class="example">
fuentes_unidas = cogroup fuente1 by (keys) fuente2 by (keys);
</pre>


</section>
<section id="slide-sec-8-12">
<h3 id="sec-8-12">Pig: Ejemplo de JOINs y COGROUPs</h3>
<pre class="example">
# Fuentes de datos

mascotas: (dueño, mascotas)
----------------------
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)

amigos: (amigo1, amigo2)
----------------------
(Diana, Adolfo)
(Gabriel, Adolfo)
(Shanti, Paty)


COGROUP mascotas by dueño, amigos por amigo2;
---------------------------------------------
(Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)}, {(Diana, Adolfo), (Gabriel, Adolfo)})
(Paty, {(Paty, perro), (Paty, gato)}, {(Shanti, Paty)})

JOIN mascotas by dueño, amigos por amigo2;
-------------------------------------------
(Adolfo, tortuga, Diana)
(Adolfo, tortuga, Gabriel)
(Adolfo, pez, Diana)
(Adolfo, pez, Gabriel)
(Adolfo, gato, Diana)
(Adolfo, gato, Gabriel)
(Paty, perro, Shanti)
(Paty, gato, Shanti)
</pre>

</section>
<section id="slide-sec-8-13">
<h3 id="sec-8-13">Aclaraciones sobre GROUP y FLATTEN</h3>
<ul>
<li><code>FLATTEN</code> elimina un nivel anidamiento</li>

</ul>

<pre class="example">
# Datos:
# (Adolfo, (tortuga, pez, gato))
# (Paty, (perro, gato))
# FLATTEN eliminaría los bags internos
(Adolfo, tortuga)
(Adolfo, pez)
(Adolfo, gato)
(Paty, perro)
(Paty, gato)
</pre>

<ul>
<li><code>GROUP .. BY</code> organiza los <i>bags</i> en <i>bags</i></li>

</ul>
<pre class="example">
# Siguiendo con los datos anteriores de mascotas
GROUP mascotas BY dueño;

# ( Adolfo, {(Adolfo, tortuga), (Adolfo, pez), (Adolfo, gato)} )
# ( Paty, {(Paty, perro), (Paty, gato)} )
</pre>

<ul>
<li>En cierto sentido <code>FLATTEN</code> y <code>GROUP .. BY</code> son operaciones inversas
entre sí.</li>

</ul>

</section>
<section id="slide-sec-8-14">
<h3 id="sec-8-14">Tarea</h3>
<p>
Crear un <code>wordcount</code> para los archivos en <code>data</code> usando <code>Pig</code>
</p>


</section>
</section>
<section>
<section id="slide-sec-9">
<h2 id="sec-9">Hive</h2>
<div class="outline-text-2" id="text-9">
</div></section>
<section id="slide-sec-9-1">
<h3 id="sec-9-1">Hive</h3>
<ul>
<li><i>Datawarehouse</i>.</li>
<li><code>HQL</code> es casi idéntico a <code>SQL</code>.</li>
<li>Proyecto de Apache.</li>
<li>Estructura a diversos formatos.</li>
<li><i>Queries</i>.</li>
<li>Acceso al <code>HDFS</code> y <code>HBASE</code>.</li>
<li><i>Queries</i> en tiempo real.</li>
<li>Facilidad de uso.</li>

</ul>

</section>
<section id="slide-sec-9-2">
<h3 id="sec-9-2">Arquitectura de Apache Hive</h3>

<div class="figure">
<p><img src="./imagenes/hive-remote.jpeg" alt="hive-remote.jpeg" />
</p>
</div>

</section>
<section id="slide-sec-9-3">
<h3 id="sec-9-3">Ejercicio: Crear RITA en Hive</h3>
<pre class="example">
CREATE EXTERNAL TABLE carriers(
code STRING,
description STRING
)
COMMENT 'Códigos de carriers'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

CREATE EXTERNAL TABLE airports(
iata STRING,
airport STRING,
city STRING,
state STRING,
country STRING,
latitude FLOAT,
longitude FLOAT
)
COMMENT 'Códigos y localización de aeropuertos'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

CREATE EXTERNAL TABLE planes_data(
tailnum STRING,
type STRING,
manufacturer STRING,
issue_date STRING,
model STRING,
status STRING,
aircraft_type STRING,
engine_type STRING,
year STRING
)
COMMENT 'Datos de algunos aviones mencionados en RITA'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;
</pre>


</section>
<section id="slide-sec-9-4">
<h3 id="sec-9-4">Ejercicio: Crear RITA en Hive</h3>
<pre class="example">
CREATE EXTERNAL TABLE rita(
Year STRING,
Month STRING,
DayofMonth STRING,
DayOfWeek STRING,
DepTime STRING,
CRSDepTime STRING,
ArrTime STRING,
CRSArrTime STRING,
UniqueCarrier STRING,
FlightNum STRING,
TailNum STRING,
ActualElapsedTime INT,
CRSElapsedTime INT,
AirTime INT,
ArrDelay INT,
DepDelay INT,
Origin STRING,
Dest STRING,
Distance FLOAT,
TaxiIn INT,
TaxiOut INT,
Cancelled INT,
CancellationCode STRING,
Diverted INT,
CarrierDelay INT,
WeatherDelay INT,
NASDelay INT,
SecurityDelay INT,
LateAircraftDelay INT
)
COMMENT 'Base de datos conteniendo los vuelos de 1987 a 2008'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION '/user/hive/rita';
</pre>

</section>
<section id="slide-sec-9-5">
<h3 id="sec-9-5">Ejercicio: RITA y HIVE</h3>
<pre class="example">
-- ¿Se crearon bien las tablas?
show tables;

-- ¿Se cargó bien rita?
select * from rita limit 5;

-- Cargamos airports
load data inpath 'hive/datawarehouse/rita/catalogs/airports.csv'
overwrite into table airports;

-- Probamos
select * from airports where iata='SAN';

-- ¿Y si hacemos un JOIN?

select * from rita join airports on (rita.Origin = airports.iata) limit 10;
-- ¿Qué pasó?
</pre>

</section>
<section id="slide-sec-9-6">
<h3 id="sec-9-6">Tarea</h3>
<p>
Crear un <code>wordcount</code> para los archivos en <code>data</code> usando <code>Hive</code>
</p>



</section>
</section>
<section>
<section id="slide-sec-10">
<h2 id="sec-10">Ejercicios</h2>
<div class="outline-text-2" id="text-10">
</div></section>
<section id="slide-sec-10-1">
<h3 id="sec-10-1">RITA, HIVE y PIG</h3>
<pre class="example">
register /home/hduser/hadoop-src/pig-0.12.0/contrib/piggybank/java/piggybank.jar;
define replace org.apache.pig.piggybank.evaluation.string.REPLACE;
define substring org.apache.pig.piggybank.evaluation.string.SUBSTRING;
define s_split org.apache.pig.piggybank.evaluation.string.Split;
define reverse org.apache.pig.piggybank.evaluation.string.Reverse;

airports = LOAD '/user/nano/rita_catalogs/airports.csv'
USING PigStorage(',')
AS
(iata:chararray,airport:chararray,city:chararray,
state:chararray,country:chararray,latitude:float,longitude:float);

fixed_airports = foreach airports
                 generate replace(iata, '"', ''),
                          replace(airport, '"', ''),
                          replace(city, '"', ''),
                          replace(state, '"', ''),
                          replace(country, '"', ''),
                          latitude, longitude;

store fixed_airports into '/user/pig/airports-fixed' using PigStorage(',');
</pre>

</section>
<section id="slide-sec-10-2">
<h3 id="sec-10-2">RITA y HIVE: Joins</h3>
<pre class="example">
load data inpath 'pig_fixed/airports/part-m-00000'
overwrite into table airports;

-- ¿Y ahora?
select * from airports where iata='SAN';

select * from rita join airports on (rita.Origin = airports.iata) limit 10;
</pre>



</section>
<section id="slide-sec-10-3">
<h3 id="sec-10-3">Tarea: Pig y Hive</h3>
<ul>
<li>Crear una tabla de RITA limpia (usando <code>PIG</code> y <code>HIVE</code>)</li>
<li>Ejecutar dos exploraciones de las tareas de analítica de PostgreSQL,
uno usando <code>PIG</code> y otro usando <code>HIVE</code>.</li>

</ul>

</section>
<section id="slide-sec-10-4">
<h3 id="sec-10-4">¡Más ejercicios!</h3>
<ul>
<li>Usando <code>RITA</code> (lo que tengan cargado en su nodo), calcule:
<ul>
<li>Con <code>Pig</code>:
<ul>
<li>El número de vuelos por aeropuerto.</li>
<li>¿Cuál es el más activo?</li>

</ul></li>
<li>Con <code>Hive</code>:
<ul>
<li>Número de <code>km</code> por avión.</li>
<li>¿Cuál es el <i>top</i> 5?</li>
<li>¿Sería más fácil en <code>Pig</code>?</li>

</ul></li>

</ul></li>

</ul>





</section>
</section>
<section>
<section id="slide-sec-11">
<h2 id="sec-11">HCatalog</h2>
<div class="outline-text-2" id="text-11">
</div></section>
<section id="slide-sec-11-1">
<h3 id="sec-11-1">HCatalog</h3>
<ul>
<li>Está incorporado a <code>Hive</code> desde la versión <code>0.11</code>.</li>

<li>Es una capa administrativa de tablas y almacenamiento que permite
que diferentes herramientas de procesamiento de datos (<code>Pig</code>,
<code>MapReduce</code>) puedan leer y escribir más fácilmente del <code>HDFS</code>.</li>
<li>Contiene una abstracción que presenta una vista relacional de los
datos contenidos en el <code>HDFS</code>, asegurando que los usuarios no se
preocupen dónde o en que formato están almacenados los datos.</li>

</ul>

</section>
<section id="slide-sec-11-2">
<h3 id="sec-11-2">HCatalog</h3>
<ul>
<li>Utiliza el <code>DDL</code> de <code>Hive</code>.</li>
<li>Provee interfaces de escritura y lectura para <code>Pig</code>, <code>MapReduce</code> y
<code>Hive</code>.</li>
<li>Usa la línea de comandos para manejar la definición de los datos y
metadatos.</li>
<li><code>HCatalog</code> presenta los datos de manera relacional.</li>
<li>Los datos son guardados en tablas y las tablas en bases de datos.</li>

<li><code>WebHCat</code> es la interfaz API <code>REST</code> de <code>HCatalog</code>.</li>

</ul>

</section>
<section id="slide-sec-11-3">
<h3 id="sec-11-3">HCatalog: Flujo de datos</h3>
<ul>
<li>Usuario 1 copia datos al HDFS</li>

</ul>
<pre class="example">
hadoop distcp file:///data/books/pg2047.txt hdfs://data/20140430/books
hcat "alter table books add partition (ds='20140430') location 'hdfs://data/20140430/books'"
</pre>

<ul>
<li>Usuario 2 usa <code>Pig</code> para limpiar y preparar los datos.
<ul>
<li><code>HCatalog</code> mandará al <code>JMS</code> un mensaje de que la información está disponible.</li>

</ul></li>

</ul>

<pre class="example">
A = load 'books' using HCatLoader();
B = filter A by date = '20140430';
...
store Z into 'procesados' using HCatStorer("date=20140430");
</pre>

<ul>
<li>Usuario 3 realiza cierta analítica</li>

</ul>
<pre class="example">
select col1, count(col3)
from procesados
where date  = '201340430'
group by col1;
</pre>

</section>
</section>
<section>
<section id="slide-sec-12">
<h2 id="sec-12">HBase</h2>
<div class="outline-text-2" id="text-12">
</div></section>
<section id="slide-sec-12-1">
<h3 id="sec-12-1">CAP Theorem</h3>

<div class="figure">
<p><img src="./imagenes/cap.png" alt="cap.png" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-sec-13">
<h2 id="sec-13">Apache Sqoop</h2>
<div class="outline-text-2" id="text-13">
</div></section>
<section id="slide-sec-13-1">
<h3 id="sec-13-1">Apache Sqoop</h3>
<ul>
<li>Herramienta para importar eficientemente <i>data</i> desde <code>RDBMS</code> a Hadoop (<code>HDFS,
  Hive, Hbase</code>) y viceversa.</li>
<li>Soporta cualquier <code>RDBMS</code> que tenga conexión <code>JDBC</code> (<code>PostgreSQL, MySQL, Oracle, Teradata</code>, etc.).</li>
<li>Tiene soporte nativo para <code>MySQL</code> y <code>PostgreSQL</code>.</li>

</ul>

</section>
<section id="slide-sec-13-2">
<h3 id="sec-13-2">Apache Sqoop</h3>

<div class="figure">
<p><img src="./imagenes/sqoop.png" alt="sqoop.png" />
</p>
</div>

</section>
<section id="slide-sec-13-3">
<h3 id="sec-13-3">Ejercicio: RITA del tingo al tango</h3>
<p>
y
</p>
</section>
</section>
<section>
<section id="slide-sec-14">
<h2 id="sec-14">Apache Flume</h2>

</section>
</section>
<section>
<section id="slide-sec-15">
<h2 id="sec-15">Apache Spark</h2>

</section>
</section>
<section>
<section id="slide-sec-16">
<h2 id="sec-16">Oozie</h2>

</section>
</section>
<section>
<section id="slide-sec-17">
<h2 id="sec-17">Hue</h2>
<div class="outline-text-2" id="text-17">
</div></section>
<section id="slide-sec-17-1">
<h3 id="sec-17-1">Ejercicio: Armar un <i>cluster</i></h3>
<ul>
<li>El objetivo es reproducir el siguiente diagrama arquitectónico (por
lo menos).</li>

</ul>


<div class="figure">
<p><img src="./imagenes/layout.png" alt="layout.png" />
</p>
</div>


<ul>
<li>Use <code>Vagrant</code>, <code>chef</code> y <code>berkshelf</code>.</li>

</ul>


</section>
</section>
<section>
<section id="slide-sec-18">
<h2 id="sec-18">Compresores</h2>
<div class="outline-text-2" id="text-18">
</div></section>
<section id="slide-sec-18-1">
<h3 id="sec-18-1">Instalación</h3>
<pre class="example">
# Snappy
sudo apt-get install libsnappy1 libsnappy-dev

#LZO
sudo apt-get install liblzo2-2 liblzo2-dev#+end_example
</pre>

</section>
<section id="slide-sec-18-2">
<h3 id="sec-18-2">Tipos</h3>

<div class="figure">
<p><img src="./imagenes/compresores.png" alt="compresores.png" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-sec-19">
<h2 id="sec-19">Misceláneos</h2>
<div class="outline-text-2" id="text-19">
</div></section>
<section id="slide-sec-19-1">
<h3 id="sec-19-1">Tips</h3>
<ul>
<li><code>Reduce</code> es regularmente más intensivo en cuanto consumo de recursos que <code>Map</code>
<ul>
<li>Usa <code>Combiners</code>.</li>
<li>Explora tus datos antes
<ul>
<li>Como están distribuidos es muy importante.</li>
<li>Quizá Hadoop no sea lo correcto.</li>

</ul></li>

</ul></li>

<li>En la vida real, instala desde una distribución: <b>BigTop</b>, <b>Horton</b> o <b>Cloudera</b>.
<ul>
<li>Y <code>Vagrant</code></li>

</ul></li>

</ul>

</section>
<section id="slide-sec-19-2">
<h3 id="sec-19-2"><i>Small File Problem</i></h3>

</section>
</section>
<section>
<section id="slide-sec-20">
<h2 id="sec-20">Disclaimer</h2>
<p>
Algunas imágenes se tomaron de los libros <i>Professional Hadoop Solutions</i>
de <b>Wrox</b> y de la página de <a href="http://hortonworks.com/hadoop/yarn/">Hortonworks</a>. Las otras son mías.
</p>

<p>
Las tablas de la sección <i>cluster</i> de Hadoop, se tomaron de <a href="http://hortonworks.com/">Hortonworks.</a>
</p>
</section>
</section>
</div>
</div>
<p> Creado por Adolfo De Unánue Tiscareño. </p>

<script src="http://cdn.jsdelivr.net/reveal.js/2.5.0/lib/js/head.min.js"></script>
<script src="http://cdn.jsdelivr.net/reveal.js/2.5.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: true,
rollingLinks: true,
keyboard: true,
overview: true,
width: 1200,
height: 800,
margin: 0.10,
minScale: 0.50,
maxScale: 2.50,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'fade', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'http://cdn.jsdelivr.net/reveal.js/2.5.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
]
});
</script>
</body>
</html>
